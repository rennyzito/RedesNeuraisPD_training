# -*- coding: utf-8 -*-
"""
Interface Streamlit para Predi√ß√£o de League of Legends
Aplica√ß√£o interativa que permite gerar valores aleat√≥rios e predizer se o time azul ganhar√° a partida
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss
import warnings
warnings.filterwarnings('ignore')



# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="League of Legends - Predi√ß√£o de Vit√≥ria",
    page_icon="‚öîÔ∏è",
    layout="wide"
)

@st.cache_data
def load_data():
    """Carrega os dados do League of Legends, altere o base_dir para o diret√≥rio onde est√° a base"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    data_file = os.path.join(base_dir, "high_diamond_ranked_10min.csv")
    
    if not os.path.exists(data_file):
        st.error(f"Arquivo de dados n√£o encontrado: {data_file}")
        return None
    
    df = pd.read_csv(data_file)
    return df

def train_mlp_with_validation_loss(X_train, X_val, y_train, y_val, hidden_layer_sizes=(100,), alpha=0.001, learning_rate_init=0.01, max_iter=500, random_state=42):
    """
    Treina um MLP manualmente e calcula training e validation loss a cada √©poca
    """
    from sklearn.preprocessing import StandardScaler
    from sklearn.neural_network import MLPClassifier
    
    # Padronizar dados
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # Criar modelo MLP
    mlp = MLPClassifier(
        hidden_layer_sizes=hidden_layer_sizes,
        alpha=alpha,
        learning_rate_init=learning_rate_init,
        max_iter=1,  # Treinar uma √©poca por vez
        warm_start=True,
        random_state=random_state
    )
    
    training_losses = []
    validation_losses = []
    
    # Treinar √©poca por √©poca
    for epoch in range(max_iter):
        # Treinar uma √©poca
        mlp.fit(X_train_scaled, y_train)
        
        # Calcular probabilidades
        train_proba = mlp.predict_proba(X_train_scaled)
        val_proba = mlp.predict_proba(X_val_scaled)
        
        # Calcular loss (log loss)
        train_loss = log_loss(y_train, train_proba)
        val_loss = log_loss(y_val, val_proba)
        
        training_losses.append(train_loss)
        validation_losses.append(val_loss)
        
        # Verificar converg√™ncia (crit√©rio simples)
        if epoch > 10 and abs(training_losses[-1] - training_losses[-2]) < 1e-6:
            break
    
    # Criar pipeline final
    final_model = Pipeline([
        ('scaler', scaler),
        ('classifier', mlp)
    ])
    
    return final_model, training_losses, validation_losses

@st.cache_resource
def train_model(df):
    """Treina o modelo de Machine Learning e retorna dados para o roteiro"""
    
    # Identificar features num√©ricas, excluindo vari√°veis do time red
    feature_cols = [col for col in df.columns 
                    if col not in ['gameId', 'blueWins'] and 
                    'red' not in col.lower() and 
                    pd.api.types.is_numeric_dtype(df[col])]
    
    # An√°lise de relev√¢ncia das features
    X_temp = df[feature_cols].fillna(df[feature_cols].median())
    y_temp = df['blueWins'].values
    
    # Mutual Information para sele√ß√£o de features
    mi_scores = mutual_info_classif(X_temp, y_temp, random_state=42)
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': mi_scores
    }).sort_values('importance', ascending=False)
    
    # Priorizar goldDiff e expDiff como vari√°veis primordiais
    primary_features = ['blueGoldDiff', 'blueExperienceDiff']
    
    # Selecionar top 3 outras features por import√¢ncia (excluindo as primordiais)
    other_features = feature_importance[~feature_importance['feature'].isin(primary_features)]
    top_3_others = other_features.head(3)['feature'].values
    
    # Combinar features primordiais com as top 3 por import√¢ncia
    selected_features = np.concatenate([primary_features, top_3_others])
    
    # Preparar dados
    X = df[selected_features].copy()
    y = df['blueWins'].copy()
    X = X.fillna(X.median())
    
    # Split dos dados
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ===== MODELO BASELINE: REGRESS√ÉO LOG√çSTICA =====
    baseline_model = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(random_state=42, max_iter=1000))
    ])
    
    baseline_model.fit(X_train, y_train)
    
    # ===== MODELO COMPLEXO: MLP COM GRIDSEARCHCV =====
    mlp_model = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', MLPClassifier(random_state=42, max_iter=500))
    ])
    
    # Hiperpar√¢metros para otimiza√ß√£o do MLP (mesmo que apresentacao_melhorado.ipynb)
    param_grid = {
        'classifier__hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],
        'classifier__alpha': [0.0001, 0.001, 0.01],
        'classifier__learning_rate_init': [0.0001, 0.001]
    }
    
    # Grid Search para MLP
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    grid_search = GridSearchCV(
        mlp_model, param_grid, cv=cv, scoring='f1',
        n_jobs=-1, verbose=0
    )
    
    # Treinamento
    grid_search.fit(X_train, y_train)
    best_mlp = grid_search.best_estimator_
    
    # ===== TREINAR MELHOR MLP COM VALIDATION LOSS =====
    # Dividir dados de treino em treino/valida√ß√£o para calcular validation loss
    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    # Treinar o melhor modelo encontrado com monitoramento de loss
    best_params_dict = grid_search.best_params_
    best_model_with_losses, training_losses, validation_losses = train_mlp_with_validation_loss(
        X_train_split, X_val_split, y_train_split, y_val_split,
        hidden_layer_sizes=best_params_dict['classifier__hidden_layer_sizes'],
        alpha=best_params_dict['classifier__alpha'],
        learning_rate_init=best_params_dict['classifier__learning_rate_init'],
        max_iter=300,
        random_state=42
    )
    
    # ===== COMPARAR MODELOS =====
    # Avaliar baseline
    baseline_pred = baseline_model.predict(X_test)
    baseline_f1 = f1_score(y_test, baseline_pred)
    baseline_accuracy = accuracy_score(y_test, baseline_pred)
    
    # Avaliar MLP otimizado
    mlp_pred = best_mlp.predict(X_test)
    mlp_f1 = f1_score(y_test, mlp_pred)
    mlp_accuracy = accuracy_score(y_test, mlp_pred)
    
    # SOLU√á√ÉO 1: Sempre usar o MLP para garantir que o gr√°fico de loss seja exibido
    # Usar o melhor modelo MLP com hist√≥rico de loss
    final_model = best_model_with_losses
    y_pred = final_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    model_type = "MLP Otimizado"
    
    # Manter informa√ß√µes de compara√ß√£o para fins informativos
    comparison_info = {
        'mlp_better': mlp_f1 > baseline_f1,
        'baseline_f1': baseline_f1,
        'mlp_f1': mlp_f1,
        'baseline_accuracy': baseline_accuracy,
        'mlp_accuracy': mlp_accuracy
    }
    
    # Calcular estat√≠sticas das features para gera√ß√£o de valores aleat√≥rios
    feature_stats = {}
    for feature in selected_features:
        feature_stats[feature] = {
            'min': float(X[feature].min()),
            'max': float(X[feature].max()),
            'mean': float(X[feature].mean()),
            'std': float(X[feature].std())
        }
    
    # Calcular matriz de confus√£o
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    # Dados para o roteiro de apresenta√ß√£o
    presentation_data = {
        'feature_cols': feature_cols,
        'feature_importance': feature_importance,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'y_pred': y_pred,
        'confusion_matrix': conf_matrix,
        'training_losses': training_losses,
        'validation_losses': validation_losses,
        'accuracy': accuracy,
        'f1_score': f1,
        'baseline_f1': baseline_f1,
        'mlp_f1': mlp_f1,
        'baseline_accuracy': baseline_accuracy,
        'mlp_accuracy': mlp_accuracy,
        'selected_model': model_type
    }
    
    return final_model, selected_features, feature_stats, grid_search.best_params_, presentation_data

def show_presentation_section(df, features, presentation_data, best_params):
    """Exibe o roteiro de apresenta√ß√£o de Deep Learning"""
    
    st.header("üìã AN√ÅLISE DEEP LEARNING")
    st.markdown("---")
    
    # Membros do Grupo
    st.subheader("üë• Membros do Grupo")
    st.markdown("""
    - Fabr√≠cio Ara√∫jo
    - Guilherme Palmiro Sardiva
    - Jean Paulo De Brum
    - Jorge Bernardo
    - Juan Emmanuel
    - Rennan da Hora
    """)

    
    # Problema de Neg√≥cio
    st.subheader("üéØ Problema de Neg√≥cio")
    st.markdown("""
    **Tipo de Problema**: CLASSIFICA√á√ÉO BIN√ÅRIA
    
    **Objetivo**: Predizer se o time Azul vencer√° uma partida de League of Legends com base nos dados coletados nos primeiros 10 minutos de jogo.
    
    **Vari√°vel Alvo**: `blueWins` (0 = derrota, 1 = vit√≥ria)
    """)
    
    # Base de Dados
    st.subheader("üìä Base de Dados")
    n_rows, n_cols = df.shape
    victory_rate = df['blueWins'].mean() * 100
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Tamanho da Amostra", f"{n_rows:,} partidas")
    with col2:
        st.metric("Total de Colunas", f"{n_cols} features")
    with col3:
        st.metric("Taxa de Vit√≥rias Azul", f"{victory_rate:.1f}%")
    
    st.markdown(f"""
    **Features/Atributos dispon√≠veis**: {n_cols-1} vari√°veis (excluindo target)
    
    **Fonte**: Dataset de partidas ranqueadas de League of Legends em alto elo (Diamond+)
    
    **Per√≠odo analisado**: Primeiros 10 minutos de cada partida
    """)
    
    # An√°lise Explorat√≥ria
    st.subheader("üîç An√°lise Explorat√≥ria")
    
    # Distribui√ß√£o de Y
    st.write("**Distribui√ß√£o da Vari√°vel Alvo (Y)**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Gr√°fico de pizza
        fig, ax = plt.subplots(figsize=(6, 4))
        counts = df['blueWins'].value_counts()
        ax.pie(counts.values, labels=['Derrota (0)', 'Vit√≥ria (1)'], 
               autopct='%1.1f%%', startangle=90)
        ax.set_title('Distribui√ß√£o da Vari√°vel Alvo')
        st.pyplot(fig)
        plt.close()
    
    with col2:
        # Informa√ß√µes detalhadas
        defeats = df['blueWins'].value_counts()[0]
        victories = df['blueWins'].value_counts()[1]
        st.markdown(f"""
        **Estat√≠sticas da Vari√°vel Alvo:**
        - Derrotas (0): {defeats:,} partidas ({(defeats/n_rows)*100:.1f}%)
        - Vit√≥rias (1): {victories:,} partidas ({(victories/n_rows)*100:.1f}%)
        
        **Balanceamento**: {'Classes balanceadas' if abs(victories - defeats) < n_rows*0.1 else 'Classes desbalanceadas'}
        """)
    
    # Distribui√ß√£o de X (Features mais importantes)
    st.write("**Distribui√ß√£o das Features Principais (X)**")
    
    # Top 5 features mais importantes
    top_features = presentation_data['feature_importance'].head(5)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Gr√°fico de import√¢ncia das features
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.barh(top_features['feature'], top_features['importance'])
        ax.set_xlabel('Import√¢ncia (Mutual Information)')
        ax.set_title('Top 5 Features por Import√¢ncia')
        st.pyplot(fig)
        plt.close()
    
    with col2:
        # Histograma das features selecionadas
        fig, axes = plt.subplots(2, 2, figsize=(8, 6))
        axes = axes.flatten()
        
        for i, feature in enumerate(features[:4]):
            if i < 4:
                axes[i].hist(df[feature].dropna(), bins=30, alpha=0.7)
                axes[i].set_title(f'{feature}')
                axes[i].set_ylabel('Frequ√™ncia')
        
        plt.tight_layout()
        st.pyplot(fig)
        plt.close()
    
    # Atributos Relevantes
    st.subheader("‚≠ê Atributos Mais Relevantes")
    
    st.markdown("""
    **Justificativa para Sele√ß√£o de Features:**
    
    1. **Features Primordiais Selecionadas:**
       - `blueGoldDiff`: Diferen√ßa de ouro entre times (indicador econ√¥mico crucial)
       - `blueExperienceDiff`: Diferen√ßa de experi√™ncia (indicador de desenvolvimento dos campe√µes)
    
    2. **M√©todo de Sele√ß√£o**: Mutual Information Score
       - Mede a depend√™ncia entre cada feature e a vari√°vel alvo
       - Identifica rela√ß√µes n√£o-lineares entre vari√°veis
    """)
    
    # Top 10 features
    st.write("**Top 10 Features por Import√¢ncia:**")
    top_10_features = presentation_data['feature_importance'].head(10)
    
    for idx, (_, row) in enumerate(top_10_features.iterrows(), 1):
        st.write(f"{idx}. **{row['feature']}**: {row['importance']:.4f}")
    
    # Matriz de Confus√£o
    st.subheader("üìä Matriz de Confus√£o")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Plotar matriz de confus√£o
        fig, ax = plt.subplots(figsize=(8, 6))
        conf_matrix = presentation_data['confusion_matrix']
        
        # Criar heatmap da matriz de confus√£o
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax,
                   xticklabels=['Derrota (0)', 'Vit√≥ria (1)'],
                   yticklabels=['Derrota (0)', 'Vit√≥ria (1)'])
        ax.set_xlabel('Predi√ß√£o')
        ax.set_ylabel('Valor Real')
        ax.set_title('Matriz de Confus√£o - Modelo MLP')
        
        st.pyplot(fig)
        plt.close()
    
    with col2:
        # Interpretar a matriz de confus√£o
        tn, fp, fn, tp = conf_matrix.ravel()
        
        st.markdown("**Interpreta√ß√£o da Matriz:**")
        st.write(f"‚úÖ **Verdadeiros Negativos (TN)**: {tn}")
        st.write(f"‚ùå **Falsos Positivos (FP)**: {fp}")
        st.write(f"‚ùå **Falsos Negativos (FN)**: {fn}")
        st.write(f"‚úÖ **Verdadeiros Positivos (TP)**: {tp}")
        
        # Calcular m√©tricas adicionais
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        st.markdown("**M√©tricas Derivadas:**")
        st.write(f"üéØ **Precis√£o**: {precision:.3f}")
        st.write(f"üìà **Recall (Sensibilidade)**: {recall:.3f}")
        st.write(f"üîç **Especificidade**: {specificity:.3f}")
    
    # Representa√ß√£o Escolhida
    st.subheader("üé® Representa√ß√£o Escolhida")
    st.markdown(f"""
    **Estrat√©gia de Sele√ß√£o de Colunas:**
    
    - **Total de features originais**: {len(presentation_data['feature_cols'])} vari√°veis
    - **Features selecionadas**: {len(features)} vari√°veis
    - **Crit√©rio**: Combina√ß√£o de conhecimento de dom√≠nio + import√¢ncia estat√≠stica
    
    **Colunas Consideradas no Modelo Final:**
    """)
    
    for i, feature in enumerate(features, 1):
        importance_score = presentation_data['feature_importance'][
            presentation_data['feature_importance']['feature'] == feature
        ]['importance'].iloc[0]
        st.write(f"{i}. `{feature}` (import√¢ncia: {importance_score:.4f})")
    
    # Treinamento
    st.subheader("üß† Treinamento")
    
    topology = best_params['classifier__hidden_layer_sizes']
    alpha = best_params['classifier__alpha']
    learning_rate = best_params['classifier__learning_rate_init']
    
    st.markdown("""
    **Estrat√©gia de Treinamento:**
    
    **1. Modelo Baseline: Regress√£o Log√≠stica**
    - **Arquitetura**: Pipeline com StandardScaler + LogisticRegression
    - **Objetivo**: Estabelecer baseline de performance
    - **Configura√ß√£o**: max_iter=1000, random_state=42
    
    **2. Modelo Complexo: Multi-Layer Perceptron (MLP)**
    - **Arquitetura**: Pipeline com StandardScaler + MLPClassifier
    - **Otimiza√ß√£o**: GridSearchCV com 5-fold cross-validation
    - **M√©trica de sele√ß√£o**: F1-Score
    - **Grid de hiperpar√¢metros**:
      - hidden_layer_sizes: [(50,), (100,), (50, 25), (100, 50)]
      - alpha: [0.0001, 0.001, 0.01]
      - learning_rate_init: [0.001, 0.01]
    """)
    
    st.markdown(f"""
    **Melhor Configura√ß√£o MLP Encontrada:**
    - **Camadas ocultas**: {topology}
    - **Taxa de regulariza√ß√£o (Œ±)**: {alpha}
    - **Taxa de aprendizado inicial**: {learning_rate}
    - **Fun√ß√£o de ativa√ß√£o**: ReLU (padr√£o do scikit-learn)
    - **Otimizador**: Adam (padr√£o do MLPClassifier)
    """)
    
    # Compara√ß√£o de Performance
    st.markdown("**Compara√ß√£o de Performance:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(f"""
        **Baseline (Regress√£o Log√≠stica):**
        - **Acur√°cia**: {presentation_data['baseline_accuracy']:.4f} ({presentation_data['baseline_accuracy']*100:.2f}%)
        - **F1-Score**: {presentation_data['baseline_f1']:.4f}
        """)
    
    with col2:
        st.markdown(f"""
        **MLP Otimizado:**
        - **Acur√°cia**: {presentation_data['mlp_accuracy']:.4f} ({presentation_data['mlp_accuracy']*100:.2f}%)
        - **F1-Score**: {presentation_data['mlp_f1']:.4f}
        """)
    
    # Modelo Selecionado
    if presentation_data['selected_model'] == "MLP Otimizado":
        improvement = ((presentation_data['mlp_f1'] - presentation_data['baseline_f1']) / presentation_data['baseline_f1']) * 100
        st.success(f"üèÜ **Modelo Selecionado**: {presentation_data['selected_model']}")
        st.info(f"üìà **Melhoria**: {improvement:.2f}% em F1-Score comparado ao baseline")
    else:
        improvement = ((presentation_data['baseline_f1'] - presentation_data['mlp_f1']) / presentation_data['mlp_f1']) * 100
        st.success(f"üèÜ **Modelo Selecionado**: {presentation_data['selected_model']}")
        st.info(f"üìä **Resultado**: Baseline superou o MLP por {improvement:.2f}% em F1-Score")
    
    st.markdown(f"""
    **Performance Final do Modelo Selecionado:**
    - **Acur√°cia**: {presentation_data['accuracy']:.4f} ({presentation_data['accuracy']*100:.2f}%)
    - **F1-Score**: {presentation_data['f1_score']:.4f}
    - **Conjuntos**: 80% treino, 20% teste
    """)
    
    # Gr√°fico de Loss
    st.subheader("üìà Curva de Loss do Treinamento")
    
    if presentation_data['training_losses'] is not None and presentation_data['validation_losses'] is not None:
        # Criar gr√°fico de loss com training e validation
        fig, ax = plt.subplots(figsize=(12, 6))
        epochs = range(1, len(presentation_data['training_losses']) + 1)
        
        ax.plot(epochs, presentation_data['training_losses'], 'b-', linewidth=2, label='Training Loss')
        ax.plot(epochs, presentation_data['validation_losses'], 'r-', linewidth=2, label='Validation Loss')
        
        ax.set_xlabel('√âpocas')
        ax.set_ylabel('Loss')
        ax.set_title('Evolu√ß√£o do Training e Validation Loss Durante o Treinamento')
        ax.grid(True, alpha=0.3)
        ax.legend()
        
        st.pyplot(fig)
        plt.close()
        
        # Informa√ß√µes adicionais sobre o treinamento
        train_losses = presentation_data['training_losses']
        val_losses = presentation_data['validation_losses']
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown(f"""
            **Training Loss:**
            - **Total de √©pocas**: {len(train_losses)}
            - **Loss inicial**: {train_losses[0]:.4f}
            - **Loss final**: {train_losses[-1]:.4f}
            - **Redu√ß√£o**: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.2f}%
            """)
        
        with col2:
            st.markdown(f"""
            **Validation Loss:**
            - **Total de √©pocas**: {len(val_losses)}
            - **Loss inicial**: {val_losses[0]:.4f}
            - **Loss final**: {val_losses[-1]:.4f}
            - **Redu√ß√£o**: {((val_losses[0] - val_losses[-1]) / val_losses[0] * 100):.2f}%
            """)
        
        # An√°lise de overfitting/underfitting
        final_gap = abs(train_losses[-1] - val_losses[-1])
        if final_gap < 0.05:
            st.success("‚úÖ **Modelo bem balanceado**: Training e Validation Loss est√£o pr√≥ximos, indicando boa generaliza√ß√£o.")
        elif val_losses[-1] > train_losses[-1] + 0.1:
            st.warning("‚ö†Ô∏è **Poss√≠vel Overfitting**: Validation Loss significativamente maior que Training Loss.")
        else:
            st.info("‚ÑπÔ∏è **Comportamento Normal**: Pequena diferen√ßa entre Training e Validation Loss √© esperada.")
            
    else:
        st.warning("‚ö†Ô∏è Hist√≥rico de loss n√£o dispon√≠vel para este modelo.")

def generate_random_values(feature_stats):
    """Gera valores aleat√≥rios para as features baseado nas estat√≠sticas dos dados"""
    random_values = {}
    for feature, stats in feature_stats.items():
        # Gerar valor aleat√≥rio baseado na distribui√ß√£o normal dos dados
        random_value = np.random.normal(stats['mean'], stats['std'])
        # Limitar ao range min/max dos dados
        random_value = np.clip(random_value, stats['min'], stats['max'])
        random_values[feature] = random_value
    return random_values

def main():
    st.title("‚öîÔ∏è League of Legends - Predi√ß√£o de Vit√≥ria do Time Azul")
    st.markdown("---")
    
    # Sidebar com informa√ß√µes
    st.sidebar.header("‚ÑπÔ∏è Sobre o Projeto")
    st.sidebar.markdown("""
    Esta aplica√ß√£o utiliza **Machine Learning** para predizer se o time azul vencer√° uma partida de League of Legends.
    
    **Modelo**: Multi-Layer Perceptron (MLP)  
    **Dados**: Partidas ranqueadas (10 primeiros minutos)  
    **Features**: Diferen√ßas de ouro, experi√™ncia e outras estat√≠sticas
    """)
    
    # Carregar dados e treinar modelo
    df = load_data()
    if df is None:
        return
    
    with st.spinner("Carregando dados e treinando modelo..."):
        model, features, feature_stats, best_params, presentation_data = train_model(df)
    
    st.success("‚úÖ Modelo treinado com sucesso!")
    
    # Criar tabs para organizar o conte√∫do
    tab1, tab2 = st.tabs(["üìã Roteiro de Apresenta√ß√£o", "üéØ Fazer Predi√ß√µes"])
    
    with tab1:
        # Exibir roteiro de apresenta√ß√£o
        show_presentation_section(df, features, presentation_data, best_params)
    
    with tab2:
        # Exibir informa√ß√µes resumidas do modelo
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üìä Informa√ß√µes do Dataset")
            st.write(f"**Total de partidas**: {len(df):,}")
            st.write(f"**Features utilizadas**: {len(features)}")
            st.write(f"**Vit√≥rias Time Azul**: {df['blueWins'].sum():,} ({df['blueWins'].mean()*100:.1f}%)")
        
        with col2:
            st.subheader("üß† Configura√ß√£o do Modelo")
            st.write(f"**Tipo**: Multi-Layer Perceptron")
            st.write(f"**Camadas ocultas**: {best_params['classifier__hidden_layer_sizes']}")
            st.write(f"**Taxa de regulariza√ß√£o**: {best_params['classifier__alpha']}")
            st.write(f"**Taxa de aprendizado**: {best_params['classifier__learning_rate_init']}")
        
        st.markdown("---")
        
        # Se√ß√£o principal - Predi√ß√£o
        st.subheader("üéØ Fazer Predi√ß√£o")
        
        # Bot√µes de a√ß√£o
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üé≤ Gerar Valores Aleat√≥rios", type="primary"):
                st.session_state.random_values = generate_random_values(feature_stats)
        
        with col2:
            if st.button("üîÑ Limpar Valores"):
                if 'random_values' in st.session_state:
                    del st.session_state.random_values
        
        # Interface para ajustar valores
        st.subheader("‚öôÔ∏è Ajustar Valores das Features")
        
        # Inicializar valores se n√£o existirem
        if 'random_values' not in st.session_state:
            st.session_state.random_values = generate_random_values(feature_stats)
        
        # Criar inputs para cada feature
        input_values = {}
        
        for i, feature in enumerate(features):
            col1, col2 = st.columns([3, 1])
            
            with col1:
                # Slider para ajustar valor
                min_val = feature_stats[feature]['min']
                max_val = feature_stats[feature]['max']
                current_val = st.session_state.random_values[feature]
                
                input_values[feature] = st.slider(
                    f"**{feature}**",
                    min_value=min_val,
                    max_value=max_val,
                    value=current_val,
                    key=f"slider_{i}"
                )
            
            with col2:
                # Mostrar valor atual
                st.metric("Valor", f"{input_values[feature]:.2f}")
        
        st.markdown("---")
        
        # Fazer predi√ß√£o
        if st.button("üöÄ Fazer Predi√ß√£o", type="secondary"):
            # Preparar dados para predi√ß√£o
            input_data = pd.DataFrame([input_values])
            
            # Fazer predi√ß√£o
            prediction = model.predict(input_data)[0]
            probability = model.predict_proba(input_data)[0]
            
            # Mostrar resultados
            st.subheader("üìã Resultado da Predi√ß√£o")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if prediction == 1:
                    st.success("üèÜ **Time Azul VENCE!**")
                else:
                    st.error("‚ùå **Time Azul PERDE!**")
            
            with col2:
                confidence = max(probability) * 100
                st.metric("Confian√ßa", f"{confidence:.1f}%")
            
            with col3:
                prob_win = probability[1] * 100
                st.metric("Prob. de Vit√≥ria", f"{prob_win:.1f}%")
            
            # Gr√°fico de probabilidades
            st.subheader("üìä Probabilidades")
            prob_df = pd.DataFrame({
                'Resultado': ['Derrota', 'Vit√≥ria'],
                'Probabilidade': probability
            })
            st.bar_chart(prob_df.set_index('Resultado'))
            
            # Mostrar valores das features utilizadas
            with st.expander("üîç Ver Valores das Features Utilizadas"):
                for feature, value in input_values.items():
                    st.write(f"**{feature}**: {value:.3f}")
    
    st.markdown("---")


if __name__ == "__main__":
    main()